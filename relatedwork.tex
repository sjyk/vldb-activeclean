
\section{Related Work}\label{rw}
\noindent \textbf{Data Cleaning: } 
There are a number of other works that use machine learning to improve the efficiency and/or reliability of data cleaning~\cite{DBLP:journals/pvldb/YakoutENOI11,yakout2013don,gokhale2014corleone}.
For example, Yakout et al. train a model that evaluates the likelihood of a proposed replacement value \cite{yakout2013don}.
Another application of machine learning is value imputation, where a missing value is predicted based on those records without missing values.
Machine learning is also increasingly applied to make automated repairs more reliable with human validation \cite{DBLP:journals/pvldb/YakoutENOI11}.
Human input is often expensive and impractical to apply to entire large datasets.
Machine learning can extrapolate rules from a small set of examples cleaned by a human (or humans) to uncleaned data \cite{gokhale2014corleone, DBLP:journals/pvldb/YakoutENOI11}.
This approach can be coupled with active learning \cite{DBLP:journals/pvldb/MozafariSFJM14} to learn an accurate model with the fewest possible number of examples.
While, in spirit, \sys is similar to these approaches, it addresses a very different problem of data cleaning before user-specified modeling.
%The key new challenge in this problem is ensuring the correctness of the user's model after partial data cleaning.

SampleClean~\cite{wang1999sample} applies data cleaning to a sample of data, and estimates the results of aggregate queries.
Sampling has also been applied to estimate the number of duplicates in a relation \cite{heise2014estimating}. 
Similarly, Bergman et al. explore the problem of query-oriented data cleaning \cite{DBLP:conf/sigmod/BergmanMNT15}, where given a query, they clean data relevant to that query. 
Deshpande et al. studied data acquisition in sensor networks \cite{deshpande2004model}. They explored value of information based prioritization of data acquisition for estimating aggregate queries of sensor readings.
Similarly, Jeffery et al. \cite{DBLP:conf/pervasive/JefferyAFHW06} explored similar prioritization based on value of information.
Existing work does not explore cleaning driven by the downstream machine learning ``queries" studied in this work.
%Finally, incremental optimization methods like SGD have a connection to incremental materialized view maintenance as the argument for incremental maintenance over recomputation is similar (i.e., relatively sparse updates).
%Krishnan et al. explored how samples of materialized views can be maintained similar to how models are updated with a sample of clean data in this work \cite{krishnan2015svc}.

\vspace{0.5em}

\noindent \textbf{Stochastic Optimization and Active Learning: } Zhao and Tong recently proposed using importance sampling in conjunction with stochastic gradient descent \cite{zhao2014stochastic}. 
This line of work builds on prior results in linear algebra that show that some matrix columns are more informative than others \cite{drineas2012fast}, and Active Learning which shows that some labels are more informative that others \cite{settles2010active}.
Active Learning largely studies the problem of label acquisition \cite{settles2010active},
and recently the links between Active Learning and Stochastic optimization have been studied \cite{guillory2009active}. 


\vspace{0.5em}

\noindent \textbf{Transfer Learning and Bias Mitigation: }  
\sys has a strong link to a field called Transfer Learning and Domain Adaptation \cite{pan2010survey}. The basic idea of Transfer Learning is that suppose a model is trained on a dataset $D$ but tested on a dataset $D'$. 
Much of the complexity and contribution of \sys comes from efficiently tuning such a process for expensive data cleaning applications -- costs not studied in Transfer Learning.
%In robotics, Mahler et al. explored a calibration problem in which data was systematically corrupted \cite{DBLP:conf/case/MahlerKLSMKPWFAG14} and proposed a rule-based technique for cleaning data.
Other problems in bias mitigation (e.g., Krishnan et al. \cite{DBLP:conf/recsys/KrishnanPFG14}) have the same structure, systematically corrupted data that is feeding into a model.
In this work, we try to generalize these principles given a general dirty dataset, convex model, and data cleaning procedure.


\vspace{0.5em}

\noindent \textbf{Secure Learning: } \sys is also related to work in adversarial learning \cite{nelson2012query}, where the goal is to make models robust to adversarial data manipulation.
This line of work has extensively studied methodologies for making models private to external queries and robust to malicious labels \cite{xiaofeature}, but the data cleaning problem explores more general corruptions than just malicious labels.
One widely applied technique in this field is reject-on-negative impact, which essentially, discards data that reduces the loss function--which will not work when we do not have access to the true loss function (only the ``dirty loss"). 


